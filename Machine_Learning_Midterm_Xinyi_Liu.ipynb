{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "_7UHBqk2o8F2",
        "9YHCDyFkrYYY"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1.  Import the spam dataset and print the first six rows.  "
      ],
      "metadata": {
        "id": "dN9nEemqoK1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "RmTzDqAVCc4v"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4YaciPOBGxi",
        "outputId": "6bc38ab2-ee21-48a0-c3c3-19aaa63201b3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/Ml Spring 2023/spam_dataset.csv')"
      ],
      "metadata": {
        "id": "hQKxSh-eB5Im"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "id": "HfeEAN5mC8B6",
        "outputId": "feed5555-0c6e-4bfb-f200-92ff6c3c69cc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   word_freq_make:  word_freq_address:  word_freq_all:  word_freq_3d:  \\\n",
              "0             0.00                0.64            0.64            0.0   \n",
              "1             0.21                0.28            0.50            0.0   \n",
              "2             0.06                0.00            0.71            0.0   \n",
              "3             0.00                0.00            0.00            0.0   \n",
              "4             0.00                0.00            0.00            0.0   \n",
              "5             0.00                0.00            0.00            0.0   \n",
              "\n",
              "   word_freq_our:  word_freq_over:  word_freq_remove:  word_freq_internet:  \\\n",
              "0            0.32             0.00               0.00                 0.00   \n",
              "1            0.14             0.28               0.21                 0.07   \n",
              "2            1.23             0.19               0.19                 0.12   \n",
              "3            0.63             0.00               0.31                 0.63   \n",
              "4            0.63             0.00               0.31                 0.63   \n",
              "5            1.85             0.00               0.00                 1.85   \n",
              "\n",
              "   word_freq_order:  word_freq_mail:  ...  char_freq_;:  char_freq_(:  \\\n",
              "0              0.00             0.00  ...          0.00         0.000   \n",
              "1              0.00             0.94  ...          0.00         0.132   \n",
              "2              0.64             0.25  ...          0.01         0.143   \n",
              "3              0.31             0.63  ...          0.00         0.137   \n",
              "4              0.31             0.63  ...          0.00         0.135   \n",
              "5              0.00             0.00  ...          0.00         0.223   \n",
              "\n",
              "   char_freq_[:  char_freq_!:  char_freq_$:  char_freq_#:  \\\n",
              "0           0.0         0.778         0.000         0.000   \n",
              "1           0.0         0.372         0.180         0.048   \n",
              "2           0.0         0.276         0.184         0.010   \n",
              "3           0.0         0.137         0.000         0.000   \n",
              "4           0.0         0.135         0.000         0.000   \n",
              "5           0.0         0.000         0.000         0.000   \n",
              "\n",
              "   capital_run_length_average:  capital_run_length_longest:  \\\n",
              "0                        3.756                           61   \n",
              "1                        5.114                          101   \n",
              "2                        9.821                          485   \n",
              "3                        3.537                           40   \n",
              "4                        3.537                           40   \n",
              "5                        3.000                           15   \n",
              "\n",
              "   capital_run_length_total:  spam  \n",
              "0                        278     1  \n",
              "1                       1028     1  \n",
              "2                       2259     1  \n",
              "3                        191     1  \n",
              "4                        191     1  \n",
              "5                         54     1  \n",
              "\n",
              "[6 rows x 58 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1af00ce7-685e-4fc3-a52c-f22529f1519a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_freq_make:</th>\n",
              "      <th>word_freq_address:</th>\n",
              "      <th>word_freq_all:</th>\n",
              "      <th>word_freq_3d:</th>\n",
              "      <th>word_freq_our:</th>\n",
              "      <th>word_freq_over:</th>\n",
              "      <th>word_freq_remove:</th>\n",
              "      <th>word_freq_internet:</th>\n",
              "      <th>word_freq_order:</th>\n",
              "      <th>word_freq_mail:</th>\n",
              "      <th>...</th>\n",
              "      <th>char_freq_;:</th>\n",
              "      <th>char_freq_(:</th>\n",
              "      <th>char_freq_[:</th>\n",
              "      <th>char_freq_!:</th>\n",
              "      <th>char_freq_$:</th>\n",
              "      <th>char_freq_#:</th>\n",
              "      <th>capital_run_length_average:</th>\n",
              "      <th>capital_run_length_longest:</th>\n",
              "      <th>capital_run_length_total:</th>\n",
              "      <th>spam</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.32</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.778</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.756</td>\n",
              "      <td>61</td>\n",
              "      <td>278</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.21</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.14</td>\n",
              "      <td>0.28</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.07</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.94</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.132</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.372</td>\n",
              "      <td>0.180</td>\n",
              "      <td>0.048</td>\n",
              "      <td>5.114</td>\n",
              "      <td>101</td>\n",
              "      <td>1028</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.06</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.71</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.23</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.19</td>\n",
              "      <td>0.12</td>\n",
              "      <td>0.64</td>\n",
              "      <td>0.25</td>\n",
              "      <td>...</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.143</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.276</td>\n",
              "      <td>0.184</td>\n",
              "      <td>0.010</td>\n",
              "      <td>9.821</td>\n",
              "      <td>485</td>\n",
              "      <td>2259</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.137</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>0.31</td>\n",
              "      <td>0.63</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.537</td>\n",
              "      <td>40</td>\n",
              "      <td>191</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.85</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.00</td>\n",
              "      <td>...</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.223</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>3.000</td>\n",
              "      <td>15</td>\n",
              "      <td>54</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6 rows Ã— 58 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1af00ce7-685e-4fc3-a52c-f22529f1519a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1af00ce7-685e-4fc3-a52c-f22529f1519a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1af00ce7-685e-4fc3-a52c-f22529f1519a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  Read through the documentation of the original dataset here:\n",
        "\n",
        "http://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.names\n",
        "\n",
        "# The dependent variable is \"spam\" where one indicates that an email is spam and zero otherwise.  Which three variables in the dataset do you think will be important predictors in a model of spam?  Why?"
      ],
      "metadata": {
        "id": "p6O0U530oNs3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are several variables in the dataset that could be important predictors in a model of spam. Here are three variables that I think could be important:\n",
        "\n",
        "`'char_freq_$`': This variable measures the percentage of characters in the email that are `'$'` symbols. It is possible that spammers use `'$'` symbols frequently in their emails to catch the attention of recipients. Therefore, this variable could be a good predictor of spam.\n",
        "\n",
        "`'capital_run_length_average'`: This variable measures the average length of uninterrupted sequences of capital letters in the email. It is possible that spammers use capital letters frequently in their emails to create a sense of urgency or importance. Therefore, this variable could also be a good predictor of spam.\n",
        "\n",
        "`'word_freq_remove'`: This variable measures the percentage of words in the email that are 'remove'. It is possible that spammers frequently use the word 'remove' in their emails to instruct recipients to take some action (such as clicking on a link or replying to the email). Therefore, this variable could also be a good predictor of spam."
      ],
      "metadata": {
        "id": "_hZH8No0DMCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.  Visualize the univariate distribution of each of the variables in the previous question.  "
      ],
      "metadata": {
        "id": "20f9IjGboP-x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['char_freq_$:'].describe())\n",
        "df['char_freq_$:'].hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "urSoYhIfDc9A",
        "outputId": "3eda213b-19ee-472c-bf1a-119cc531c103"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    4601.000000\n",
            "mean        0.075811\n",
            "std         0.245882\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%         0.000000\n",
            "75%         0.052000\n",
            "max         6.003000\n",
            "Name: char_freq_$:, dtype: float64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAPoklEQVR4nO3cf4xlZX3H8fdHFoWCZVHMhOySLomkDUqqdIIYGjNCBBQj/KEGQ3U1JPsPbTElUTBpiD9IMCliJWqyYWlXS10JapagqSXAxPoHAivqCkjZ4lp2g251AR38lbXf/jHP2gnOMndn7s6duc/7lUzmnOc89znPdwife/a5555UFZKkPrxo1BOQJC0fQ1+SOmLoS1JHDH1J6oihL0kdWTPqCbyQk046qTZs2LDo1z/33HMcd9xxw5vQiIxLHWAtK9G41AHWctCOHTt+WlWvmO/Yig79DRs28OCDDy769dPT00xNTQ1vQiMyLnWAtaxE41IHWMtBSX50qGMu70hSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkdW9Ddyl2rn3md579VfXfbz7r7+omU/pyQNwit9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdGTj0kxyV5KEkd7b9U5N8K8muJF9M8uLW/pK2v6sd3zBnjGta+2NJLhh6NZKkF3Q4V/pXAo/O2f84cGNVvRJ4Gri8tV8OPN3ab2z9SHI6cCnwKuBC4DNJjlra9CVJh2Og0E+yHrgIuLntBzgXuL112Qpc0rYvbvu04+e1/hcD26rqN1X1Q2AXcNYQapAkDWjNgP0+CXwAeGnbfznwTFUdaPt7gHVtex3wJEBVHUjybOu/DrhvzphzX/N7STYBmwAmJiaYnp4ecIp/aOJYuOqMAwt3HLKlzHk+MzMzQx9zVKxl5RmXOsBaBrFg6Cd5K7CvqnYkmRr6DJ6nqjYDmwEmJydramrxp7zp1u3csHPQ97Xh2X3Z1FDHm56eZil/h5XEWlaecakDrGUQgyTiOcDbkrwFOAb4Y+AfgbVJ1rSr/fXA3tZ/L3AKsCfJGuAE4Gdz2g+a+xpJ0jJYcE2/qq6pqvVVtYHZD2LvqarLgHuBt7duG4HtbfuOtk87fk9VVWu/tN3dcypwGnD/0CqRJC1oKWsfHwS2JfkY8BCwpbVvAT6fZBewn9k3Cqrq4SS3AY8AB4Arqup3Szi/JOkwHVboV9U0MN22n2Ceu2+q6tfAOw7x+uuA6w53kpKk4fAbuZLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR1ZMPSTHJPk/iTfTfJwkg+39lOTfCvJriRfTPLi1v6Str+rHd8wZ6xrWvtjSS44YlVJkuY1yJX+b4Bzq+rPgdcAFyY5G/g4cGNVvRJ4Gri89b8ceLq139j6keR04FLgVcCFwGeSHDXEWiRJC1gw9GvWTNs9uv0UcC5we2vfClzSti9u+7Tj5yVJa99WVb+pqh8Cu4CzhlGEJGkwawbp1K7IdwCvBD4N/BfwTFUdaF32AOva9jrgSYCqOpDkWeDlrf2+OcPOfc3cc20CNgFMTEwwPT19eBXNMXEsXHXGgYU7DtlS5jyfmZmZoY85Ktay8oxLHWAtgxgo9Kvqd8BrkqwFvgL82dBn8v/n2gxsBpicnKypqalFj3XTrdu5YedAJQ7V7sumhjre9PQ0S/k7rCTWsvKMSx1gLYM4rLt3quoZ4F7g9cDaJAcTdT2wt23vBU4BaMdPAH42t32e10iSlsEgd++8ol3hk+RY4E3Ao8yG/9tbt43A9rZ9R9unHb+nqqq1X9ru7jkVOA24f0h1SJIGMMjax8nA1rau/yLgtqq6M8kjwLYkHwMeAra0/luAzyfZBexn9o4dqurhJLcBjwAHgCvaspEkaZksGPpV9T3gtfO0P8E8d99U1a+BdxxirOuA6w5/mpKkYfAbuZLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR1ZMPSTnJLk3iSPJHk4yZWt/WVJ7kryePt9YmtPkk8l2ZXke0nOnDPWxtb/8SQbj1xZkqT5DHKlfwC4qqpOB84GrkhyOnA1cHdVnQbc3fYB3gyc1n42AZ+F2TcJ4FrgdcBZwLUH3ygkSctjwdCvqqeq6ttt+xfAo8A64GJga+u2FbikbV8MfK5m3QesTXIycAFwV1Xtr6qngbuAC4dZjCTphaWqBu+cbAC+Abwa+O+qWtvaAzxdVWuT3AlcX1XfbMfuBj4ITAHHVNXHWvvfA7+qqn943jk2MfsvBCYmJv5i27Ztiy5u3/5n+cmvFv3yRTtj3QlDHW9mZobjjz9+qGOOirWsPONSB1jLQW984xt3VNXkfMfWDDpIkuOBLwHvr6qfz+b8rKqqJIO/e7yAqtoMbAaYnJysqampRY91063buWHnwCUOze7LpoY63vT0NEv5O6wk1rLyjEsdYC2DGOjunSRHMxv4t1bVl1vzT9qyDe33vta+FzhlzsvXt7ZDtUuSlskgd+8E2AI8WlWfmHPoDuDgHTgbge1z2t/T7uI5G3i2qp4Cvg6cn+TE9gHu+a1NkrRMBln7OAd4N7AzyXda24eA64HbklwO/Ah4Zzv2NeAtwC7gl8D7AKpqf5KPAg+0fh+pqv3DKEKSNJgFQ799IJtDHD5vnv4FXHGIsW4BbjmcCUqShsdv5EpSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHVkwdBPckuSfUm+P6ftZUnuSvJ4+31ia0+STyXZleR7Sc6c85qNrf/jSTYemXIkSS9kkCv9fwYufF7b1cDdVXUacHfbB3gzcFr72QR8FmbfJIBrgdcBZwHXHnyjkCQtnwVDv6q+Aex/XvPFwNa2vRW4ZE7752rWfcDaJCcDFwB3VdX+qnoauIs/fCORJB1hi13Tn6iqp9r2j4GJtr0OeHJOvz2t7VDtkqRltGapA1RVJalhTAYgySZml4aYmJhgenp60WNNHAtXnXFgSDMb3FLmPJ+ZmZmhjzkq1rLyjEsdYC2DWGzo/yTJyVX1VFu+2dfa9wKnzOm3vrXtBaae1z4938BVtRnYDDA5OVlTU1PzdRvITbdu54adS35fO2y7L5sa6njT09Ms5e+wkljLyjMudYC1DGKxyzt3AAfvwNkIbJ/T/p52F8/ZwLNtGejrwPlJTmwf4J7f2iRJy2jBy+AkX2D2Kv2kJHuYvQvneuC2JJcDPwLe2bp/DXgLsAv4JfA+gKran+SjwAOt30eq6vkfDkuSjrAFQ7+q3nWIQ+fN07eAKw4xzi3ALYc1O0nSUPmNXEnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI6sGfUExtGGq7861PGuOuMA7x1gzN3XXzTU80oaP8t+pZ/kwiSPJdmV5OrlPr8k9WxZQz/JUcCngTcDpwPvSnL6cs5Bknq23Ms7ZwG7quoJgCTbgIuBR5Z5HmNp2MtKh8OlJWl1WO7QXwc8OWd/D/C6uR2SbAI2td2ZJI8t4XwnAT9dwutXhL9dBXXk4wN3XfG1HIZxqWVc6gBrOehPDnVgxX2QW1Wbgc3DGCvJg1U1OYyxRmlc6gBrWYnGpQ6wlkEs9we5e4FT5uyvb22SpGWw3KH/AHBaklOTvBi4FLhjmecgSd1a1uWdqjqQ5K+BrwNHAbdU1cNH8JRDWSZaAcalDrCWlWhc6gBrWVCq6kiMK0lagXwMgyR1xNCXpI6MZeiPy6MektySZF+S7496LkuV5JQk9yZ5JMnDSa4c9ZwWI8kxSe5P8t1Wx4dHPaelSnJUkoeS3DnquSxFkt1Jdib5TpIHRz2fxUqyNsntSX6Q5NEkrx/q+OO2pt8e9fCfwJuY/fLXA8C7qmrVfes3yRuAGeBzVfXqUc9nKZKcDJxcVd9O8lJgB3DJavvvkiTAcVU1k+Ro4JvAlVV134intmhJ/g6YBP64qt466vksVpLdwGRVreovZyXZCvxHVd3c7nL8o6p6Zljjj+OV/u8f9VBVvwUOPuph1amqbwD7Rz2PYaiqp6rq2237F8CjzH5De1WpWTNt9+j2s2qvnJKsBy4Cbh71XARJTgDeAGwBqKrfDjPwYTxDf75HPay6cBlnSTYArwW+NeKpLEpbDvkOsA+4q6pWZR3NJ4EPAP874nkMQwH/nmRHe5zLanQq8D/AP7Ult5uTHDfME4xj6GsFS3I88CXg/VX181HPZzGq6ndV9Rpmv1F+VpJVufSW5K3AvqraMeq5DMlfVtWZzD7F94q2PLrarAHOBD5bVa8FngOG+rnkOIa+j3pYodoa+JeAW6vqy6Oez1K1f3bfC1w44qks1jnA29pa+Dbg3CT/MtopLV5V7W2/9wFfYXapd7XZA+yZ86/H25l9ExiacQx9H/WwArUPQLcAj1bVJ0Y9n8VK8ooka9v2sczeMPCDkU5qkarqmqpaX1UbmP3/5J6q+qsRT2tRkhzXbhCgLYecD6y6u96q6sfAk0n+tDWdx5AfPb/inrK5VCN41MMRk+QLwBRwUpI9wLVVtWW0s1q0c4B3AzvbejjAh6rqa6Ob0qKcDGxtd4m9CLitqlb1rY5jYgL4yuy1BWuAf62qfxvtlBbtb4Bb20XrE8D7hjn42N2yKUk6tHFc3pEkHYKhL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjryfztCqdzHIt+tAAAAAElFTkSuQmCC\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['capital_run_length_average:'].describe())\n",
        "df['capital_run_length_average:'].hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "1HpdpYVsG_Fs",
        "outputId": "3f049846-6c9a-487c-b2d9-5d582944045b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    4601.000000\n",
            "mean        5.191515\n",
            "std        31.729449\n",
            "min         1.000000\n",
            "25%         1.588000\n",
            "50%         2.276000\n",
            "75%         3.706000\n",
            "max      1102.500000\n",
            "Name: capital_run_length_average:, dtype: float64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['word_freq_remove:'].describe())\n",
        "df['word_freq_remove:'].hist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 444
        },
        "id": "8dnD0SMnHC4B",
        "outputId": "996cf92c-62e4-4085-b48c-1869d1b730fb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "count    4601.000000\n",
            "mean        0.114208\n",
            "std         0.391441\n",
            "min         0.000000\n",
            "25%         0.000000\n",
            "50%         0.000000\n",
            "75%         0.000000\n",
            "max         7.270000\n",
            "Name: word_freq_remove:, dtype: float64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<AxesSubplot:>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Name each of the supervised learning models that we have learned thus far that are used to predict dependent variables like \"spam\". "
      ],
      "metadata": {
        "id": "d6uW9kRjoVtQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to focus on supervised learning models in classification:\n",
        "\n",
        "1.K Nearest Neighbors\n",
        "\n",
        "2.Logistic Regression\n",
        "\n",
        "3.Random Forest\n",
        "\n",
        "4.Decision Tree\n",
        "\n",
        "5.Support Vector Machines\n",
        "\n",
        "6.Bagging\n",
        "\n"
      ],
      "metadata": {
        "id": "lv6Ixv7dHdZo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Describe the importance of training and test data.  Why do we separate data into these subsets?"
      ],
      "metadata": {
        "id": "Cx1ggR9LoXyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The importance of training and test data is that it allows us to assess the performance of a machine learning model on new, unseen data. When we train a machine learning model, we fit the model to a set of training data, which consists of input features and corresponding output labels. The goal of the model is to learn patterns in the training data that can be used to predict the output label for new, unseen input features.\n",
        "\n",
        "However, we cannot rely solely on the model's performance on the training data to assess its effectiveness. This is because the model may have simply memorized the training data, rather than learning generalizable patterns that can be applied to new data. To test the model's performance on new data, we need to evaluate it on a separate set of data that the model has not seen before. This is typically done using a test data set, which consists of input features and output labels that are not used in the training process.\n",
        "\n",
        "By separating the data into training and test sets, we can measure how well the model generalizes to new data. If the model performs well on the training data but poorly on the test data, it may be overfitting, which means it is too complex and has learned the noise in the training data. In contrast, if the model performs poorly on both the training and test data, it may be underfitting, which means it is too simple and is unable to capture the patterns in the data."
      ],
      "metadata": {
        "id": "1E8WvzigH-2I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. What is k-fold cross validation and what do we use it for?"
      ],
      "metadata": {
        "id": "_7UHBqk2o8F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-fold cross validation is a technique used to evaluate the performance of a machine learning model by partitioning the data into k equal-sized subsets (or \"folds\"). The model is then trained on k-1 of the folds and evaluated on the remaining fold. This process is repeated k times, with each fold used exactly once as the validation data. The performance metrics from each iteration are then averaged to obtain an estimate of the model's generalization performance.\n",
        "\n",
        "The purpose of k-fold cross validation is to provide a more accurate estimate of the model's performance than using a single training and test split. By repeating the training and evaluation process k times, we can obtain a more reliable estimate of the model's performance on unseen data. Additionally, k-fold cross validation allows us to use all the data for training and testing, which can be especially beneficial in situations where the amount of data is limited.\n",
        "\n",
        "In practice, k-fold cross validation is often used for hyperparameter tuning, which involves selecting the optimal values for model parameters that cannot be learned from the data. By evaluating the performance of the model for different hyperparameter values using k-fold cross validation, we can select the hyperparameters that yield the best performance on average across all folds. This can improve the generalization performance of the model on new, unseen data."
      ],
      "metadata": {
        "id": "K8i-JKnhISH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. How is k-fold cross validation different from stratified k-fold cross validation?"
      ],
      "metadata": {
        "id": "OEhD8ocBpBjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "They differ in how they split the data into folds.\n",
        "\n",
        "In k-fold cross-validation, the data is randomly partitioned into k equal-sized folds, and the model is trained and tested k times, each time using a different fold as the test set and the remaining folds as the training set.\n",
        "\n",
        "In stratified k-fold cross-validation, the data is split into k folds while preserving the percentage of samples for each class. This is especially useful when dealing with imbalanced datasets, where one class may have significantly fewer samples than the other(s). In such cases, stratified k-fold cross-validation can help ensure that each fold has a representative proportion of each class."
      ],
      "metadata": {
        "id": "dUHGJjDCISmm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8.1 Choose one model from question four.  Split the data into training and test subsets.  Build a model with the three variables in the dataset that you think will be good predictors of \"spam\".  "
      ],
      "metadata": {
        "id": "uAlK8AC8pXFQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## KNN model"
      ],
      "metadata": {
        "id": "-pKFQqV_pYK1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Select the three variables for the KNN model\n",
        "X = df[['char_freq_$:', 'capital_run_length_average:', 'word_freq_remove:']]\n",
        "y = df['spam']\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "id": "BITUQJDlKluL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 8.2 Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  "
      ],
      "metadata": {
        "id": "MXCcgMxhplWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "knn_param_grid = {'n_neighbors': range(1, 10)}\n",
        "knn_grid = GridSearchCV(KNeighborsClassifier(), knn_param_grid).fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameter: {}\".format(knn_grid.best_params_))\n",
        "print(\"Best Cross-Validation Score: {:.3f}\".format(knn_grid.best_score_))\n",
        "print(\"Test set Score: {:.3f}\".format(knn_grid.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AwVca3-kc1LW",
        "outputId": "e7a03a30-ce68-4790-9fbb-15655b2b117d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameter: {'n_neighbors': 8}\n",
            "Best Cross-Validation Score: 0.854\n",
            "Test set Score: 0.851\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used gridsearch to find out the best parameter for KNN--N = 8."
      ],
      "metadata": {
        "id": "1GDtEBgDHcNv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8.3 Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation.  "
      ],
      "metadata": {
        "id": "j9kDxH_6poR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Train the KNN model with k=8\n",
        "knn = KNeighborsClassifier(n_neighbors=8)\n",
        "knn.fit(X_train, y_train)\n",
        "print(\"Training set score: {:.3f}\".format(knn.score(X_train, y_train)))\n",
        "\n",
        "# Evaluate the model on the test set  \n",
        "test_acc = knn.score(X_test, y_test)\n",
        "print(\"Accuracy on test data: {:.2f}%\".format(test_acc * 100))\n",
        "\n",
        "from statistics import mean \n",
        "\n",
        "# Evaluate the model using k-fold cross-validation\n",
        "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(knn, X_train, y_train))))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuuJxT36dHrS",
        "outputId": "9830f746-78d3-401c-c71a-2760c8fb34e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/base.py:420: UserWarning: X does not have valid feature names, but KNeighborsClassifier was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set score: 0.710\n",
            "Accuracy on test data: 84.65%\n",
            "Mean Cross-Validation, Kfold: 0.854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.1 Choose a second model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\". "
      ],
      "metadata": {
        "id": "srjoJ4QepqPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## logistic regression model"
      ],
      "metadata": {
        "id": "wfIxrAwzg_FR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.2 Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k).  "
      ],
      "metadata": {
        "id": "mGCa0WRFpuh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Define the parameter grid\n",
        "logreg_param_grid = {'C': [0.1, 1, 10], 'penalty': ['none', 'l1', 'l2']}\n",
        "\n",
        "# Create a logistic regression model\n",
        "logreg = LogisticRegression()\n",
        "\n",
        "# Create a grid search object\n",
        "logreg_grid = GridSearchCV(logreg, logreg_param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search object to the data\n",
        "logreg_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameter: {}\".format(logreg_grid.best_params_))\n",
        "print(\"Best Cross-Validation Score: {:.3f}\".format(logreg_grid.best_score_))\n",
        "print(\"Test set Score: {:.3f}\".format(logreg_grid.score(X_test, y_test)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7seDcwM2jFyN",
        "outputId": "44c30e2f-c6c2-4d21-eb07-a77302c70125"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameter: {'C': 1, 'penalty': 'l2'}\n",
            "Best Cross-Validation Score: 0.841\n",
            "Test set Score: 0.823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "15 fits failed out of a total of 45.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.84006211        nan 0.83012422 0.84006211        nan 0.84068323\n",
            " 0.84006211        nan 0.84068323]\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used gridsearch to find out the best parameter for logistic regression--{'C': 1, 'penalty': 'l2'}"
      ],
      "metadata": {
        "id": "UyiD9ZSDHmHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.3  Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. "
      ],
      "metadata": {
        "id": "-FfYoOndp6e2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the logistic regression model\n",
        "logreg = LogisticRegression(C=0.1, penalty='none', random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "logreg.fit(X_train, y_train)\n",
        "print(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\n",
        "\n",
        "# Evaluate the model on the test set  \n",
        "test_acc = logreg.score(X_test, y_test)\n",
        "print(\"Accuracy on test data: {:.2f}%\".format(test_acc * 100))\n",
        "\n",
        "from statistics import mean \n",
        "\n",
        "# Evaluate the model using k-fold cross-validation\n",
        "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(logreg, X_train, y_train))))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4A9jzK_GhJWB",
        "outputId": "035423fe-9ef7-44a6-b2fa-f772486f8ea7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set score: 0.839\n",
            "Accuracy on test data: 82.91%\n",
            "Mean Cross-Validation, Kfold: 0.840\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9.4 Did this model predict test data better than your previous model?"
      ],
      "metadata": {
        "id": "EoFIfxinrriO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "No. The previous KNN model's accuracy on test data is 85.23%, and Mean Cross-Validation, Kfold score is 0.856, both higher than this logistic regression model."
      ],
      "metadata": {
        "id": "8froNd11gqhq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.1 Choose a third model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\". "
      ],
      "metadata": {
        "id": "QyQr5vVyqBKC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Decision Tree Classifier"
      ],
      "metadata": {
        "id": "hZgyk7uZpMLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.2 Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). "
      ],
      "metadata": {
        "id": "8YF_LD3LqCpo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# create a Decision Tree Classifier object\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# define the hyperparameter grid\n",
        "dt_params = {'criterion': ['gini', 'entropy'],\n",
        "          'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
        "          'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
        "\n",
        "# create a GridSearchCV object\n",
        "dt_grid = GridSearchCV(estimator=dt, param_grid=dt_params, cv=5, n_jobs=-1)\n",
        "\n",
        "# fit the GridSearchCV object to the data\n",
        "dt_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameter: {}\".format(dt_grid.best_params_))\n",
        "print(\"Best Cross-Validation Score: {:.3f}\".format(dt_grid.best_score_))\n",
        "print(\"Test set Score: {:.3f}\".format(dt_grid.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPll4E4po58a",
        "outputId": "7b11bdb6-318b-46e5-ff76-ca63e3c2f957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameter: {'criterion': 'gini', 'max_depth': 4, 'min_samples_split': 2}\n",
            "Best Cross-Validation Score: 0.858\n",
            "Test set Score: 0.839\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used gridsearch to find out the best parameter for decision tree-{'criterion': 'gini', 'max_depth': 4, 'min_samples_split': 2}"
      ],
      "metadata": {
        "id": "l56OdSfzHybB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.3 Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation. "
      ],
      "metadata": {
        "id": "1qEr8KgHqGl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a decision tree classifier with the given parameters\n",
        "clf = DecisionTreeClassifier(criterion='gini', max_depth=4, min_samples_split=2)\n",
        "\n",
        "# Train the model on the training data\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"Training set score: {:.3f}\".format(clf.score(X_train, y_train)))\n",
        "\n",
        "# Evaluate the model on the test set  \n",
        "test_acc = clf.score(X_test, y_test)\n",
        "print(\"Accuracy on test data: {:.2f}%\".format(test_acc * 100))\n",
        "\n",
        "from statistics import mean \n",
        "\n",
        "# Evaluate the model using k-fold cross-validation\n",
        "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(clf, X_train, y_train))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PX7Ff9hMrP2v",
        "outputId": "aa255a1c-e04d-40a1-9692-9c3db5f22727"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set score: 0.864\n",
            "Accuracy on test data: 83.85%\n",
            "Mean Cross-Validation, Kfold: 0.858\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10.4 Did this model predict test data better than your previous model?"
      ],
      "metadata": {
        "id": "9YHCDyFkrYYY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The score of previous two models are as follows:\n",
        "\n",
        "**KNN:**\n",
        "\n",
        "Training set score: 0.873\n",
        "\n",
        "Accuracy on test data: 85.23%\n",
        "\n",
        "Mean Cross-Validation, Kfold: 0.856\n",
        "\n",
        "**Logistic Regression:**\n",
        "\n",
        "Training set score: 0.839\n",
        "\n",
        "Accuracy on test data: 82.91%\n",
        "\n",
        "Mean Cross-Validation, Kfold: 0.840\n",
        "\n",
        "So decision tree classifier is better than logistic regression, slightly better than KNN on cv score but slightly worse than kNN on test data score.\n"
      ],
      "metadata": {
        "id": "LhNjrA1ygdQA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 11.1 Choose a fourth model from question four.  Using the same three variables in the dataset that you think will be good predictors of \"spam\".  "
      ],
      "metadata": {
        "id": "RPwAoH5kqMy0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random Forest"
      ],
      "metadata": {
        "id": "SBqzPc3cYKNK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.2 Describe why you chose any particular parameters for your model (e.g.- if you used KNN how did you decide to choose a specific value for k). "
      ],
      "metadata": {
        "id": "KnqsZDhzqOeW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create a random forest classifier object\n",
        "rf_clf = RandomForestClassifier()\n",
        "\n",
        "# Create a GridSearchCV object with 5-fold cross-validation\n",
        "rf_grid = GridSearchCV(rf_clf, param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "rf_grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameter: {}\".format(rf_grid.best_params_))\n",
        "print(\"Best Cross-Validation Score: {:.3f}\".format(rf_grid.best_score_))\n",
        "print(\"Test set Score: {:.3f}\".format(rf_grid.score(X_test, y_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCxL1IZDt5BL",
        "outputId": "476e9e45-f7ae-4160-d57e-bf4a9bd539d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameter: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Best Cross-Validation Score: 0.867\n",
            "Test set Score: 0.852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " used gridsearch to find out the best parameter for random forest--{'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}"
      ],
      "metadata": {
        "id": "8Ki8B109IUxX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.3 Run the model and evaluate prediction error in two ways: A) On test data directly and B) using k-fold cross-validation."
      ],
      "metadata": {
        "id": "SbIR8NQxqSk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a decision tree classifier with the given parameters\n",
        "rf = RandomForestClassifier(max_depth=5, min_samples_leaf=1, min_samples_split=10, n_estimators=200, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# Train the model on the training data\n",
        "rf.fit(X_train, y_train)\n",
        "print(\"Training set score: {:.3f}\".format(rf.score(X_train, y_train)))\n",
        "\n",
        "# Evaluate the model on the test set  \n",
        "test_acc = rf.score(X_test, y_test)\n",
        "print(\"Accuracy on test data: {:.2f}%\".format(test_acc * 100))\n",
        "\n",
        "from statistics import mean \n",
        "\n",
        "# Evaluate the model using k-fold cross-validation\n",
        "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(rf, X_train, y_train))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yycjT8PtYRJx",
        "outputId": "e7f3eb57-0d05-42f2-cdc9-186b96f4b5fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 0.8515568428674873\n",
            "Training set score: 0.864\n",
            "Accuracy on test data: 85.16%\n",
            "Mean Cross-Validation, Kfold: 0.864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11.4  Evaluate prediction error, did this model predict better than your previous models?"
      ],
      "metadata": {
        "id": "75bd4I4or223"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The scores of previous models are as follows:\n",
        "\n",
        "**KNN:**\n",
        "Training set score: 0.873\n",
        "\n",
        "Accuracy on test data: 85.23%\n",
        "\n",
        "Mean Cross-Validation, Kfold: 0.856\n",
        "\n",
        "**Logistic Regression:**\n",
        "Training set score: 0.839\n",
        "\n",
        "Accuracy on test data: 82.91%\n",
        "\n",
        "Mean Cross-Validation, Kfold: 0.840\n",
        "\n",
        "**Decision tree classifter:**\n",
        "Training set score: 0.864\n",
        "\n",
        "Accuracy on test data: 83.85%\n",
        "\n",
        "Mean Cross-Validation, Kfold: 0.858\n",
        "\n",
        "So random forest model has better performance than decision tree and logistic regression, also slightly better than KNN on cv score, but a slightly lower test score than KNN."
      ],
      "metadata": {
        "id": "CgyxvxhGgbUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 12. Now rerun your best model from questions 8 through 11, but this time add three new variables to the model that you think will increase prediction accuracy.   Did this model predict test data better than your previous models?  "
      ],
      "metadata": {
        "id": "dw-Md8j2qV-P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this dataset, cross validation score is more reliable. So I will use random forest model. This time I add three new variables: `\"word_freq_order:', \"word_freq_our:\", \"word_freq_mail:\"`"
      ],
      "metadata": {
        "id": "7CjExg2u6U1y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Select the three variables for the logistic regression model\n",
        "Xn = df[['char_freq_$:', 'capital_run_length_average:', 'word_freq_remove:', 'word_freq_order:', \"word_freq_our:\", \"word_freq_mail:\"]]\n",
        "yn = df['spam']\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "Xn_scaled = scaler.fit_transform(Xn)\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train_n, X_test_n, y_train_n, y_test_n = train_test_split(Xn_scaled, yn, test_size=0.3, random_state=42)"
      ],
      "metadata": {
        "id": "hVM7CudidYtx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid to search over\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Create a random forest classifier object\n",
        "rf_clf_n = RandomForestClassifier()\n",
        "\n",
        "# Create a GridSearchCV object with 5-fold cross-validation\n",
        "rf_grid_n = GridSearchCV(rf_clf, param_grid, cv=5, n_jobs=-1)\n",
        "\n",
        "# Fit the GridSearchCV object to the data\n",
        "rf_grid_n.fit(X_train_n, y_train_n)\n",
        "\n",
        "print(\"Best Parameter: {}\".format(rf_grid_n.best_params_))\n",
        "print(\"Best Cross-Validation Score: {:.3f}\".format(rf_grid_n.best_score_))\n",
        "print(\"Test set Score: {:.3f}\".format(rf_grid_n.score(X_test_n, y_test_n)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BkAA0siev7PW",
        "outputId": "ea0b42c8-c928-4b62-ba7d-69c23ef86c3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameter: {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 50}\n",
            "Best Cross-Validation Score: 0.882\n",
            "Test set Score: 0.870\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a decision tree classifier with the given parameters\n",
        "rfn = RandomForestClassifier(max_depth=10, min_samples_leaf=1, min_samples_split=2, n_estimators=50, random_state=42)\n",
        "\n",
        "# Fit the model to the training data\n",
        "rfn.fit(X_train_n, y_train_n)\n",
        "\n",
        "# Train the model on the training data\n",
        "rfn.fit(X_train_n, y_train_n)\n",
        "print(\"Training set score: {:.3f}\".format(rfn.score(X_train_n, y_train_n)))\n",
        "\n",
        "# Evaluate the model on the test set  \n",
        "test_acc = rfn.score(X_test_n, y_test_n)\n",
        "print(\"Accuracy on test data: {:.2f}%\".format(test_acc * 100))\n",
        "\n",
        "from statistics import mean \n",
        "\n",
        "# Evaluate the model using k-fold cross-validation\n",
        "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(rfn, X_train_n, y_train_n))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghathUREwGTp",
        "outputId": "f45884ed-40ab-4723-9ea7-4325f1887a0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on test data: 0.8740043446777698\n",
            "Training set score: 0.928\n",
            "Accuracy on test data: 87.40%\n",
            "Mean Cross-Validation, Kfold: 0.882\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Either test dataset score or cross validation score is higher than all the previous models.\n",
        "\n",
        "Before adding the three new variables:\n",
        "\n",
        "**KNN:**\n",
        "Accuracy on test data: 85.23%\n",
        "\n",
        "Mean Cross-Validation, Kfold: 0.856\n",
        "\n",
        "\n",
        "**Logistic Regression:**\n",
        "Accuracy on test data: 82.91%\n",
        "\n",
        "Mean Cross-Validation, Kfold: 0.840\n",
        "\n",
        "**Decision Tree Classifier:**\n",
        "Accuracy on test data: 83.85%\n",
        "\n",
        "Mean Cross-Validation, Kfold: 0.858\n",
        "\n",
        "\n",
        "**Random Forest:**\n",
        "Accuracy on test data: 85.16%\n",
        "\n",
        "Mean Cross-Validation, Kfold: 0.864\n",
        "\n",
        "It's obvious that random forest model with three new varibles has the highest scores:\n",
        "Accuracy on test data: 87.40%\n",
        "\n",
        "Mean Cross-Validation, Kfold: 0.882"
      ],
      "metadata": {
        "id": "1zzDoJ1AIux-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13.1 Rerun all your other models with this final set of six variables, evaluate prediction error."
      ],
      "metadata": {
        "id": "1taV2oIQqZXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.1 A KNN"
      ],
      "metadata": {
        "id": "JCiJ6Bz3q1af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "knn_param_grid = {'n_neighbors': range(1, 10)}\n",
        "knn_grid_n = GridSearchCV(KNeighborsClassifier(), knn_param_grid).fit(X_train_n, y_train_n)\n",
        "\n",
        "print(\"Best Parameter: {}\".format(knn_grid_n.best_params_))\n",
        "print(\"Best Cross-Validation Score: {:.3f}\".format(knn_grid_n.best_score_))\n",
        "print(\"Test set Score: {:.3f}\".format(knn_grid_n.score(X_test_n, y_test_n)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ0rPaPY9O0l",
        "outputId": "83388e73-026c-449a-e068-ca02e2ed7599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameter: {'n_neighbors': 5}\n",
            "Best Cross-Validation Score: 0.855\n",
            "Test set Score: 0.844\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "\n",
        "# Train the KNN model with k=5\n",
        "knn_n = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_n.fit(X_train_n, y_train_n)\n",
        "print(\"Training set score: {:.3f}\".format(knn_n.score(X_train_n, y_train_n)))\n",
        "\n",
        "# Evaluate the model on the test set  \n",
        "test_acc = knn_n.score(X_test_n, y_test_n)\n",
        "print(\"Accuracy on test data: {:.2f}%\".format(test_acc * 100))\n",
        "\n",
        "from statistics import mean \n",
        "\n",
        "# Evaluate the model using k-fold cross-validation\n",
        "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(knn_n, X_train_n, y_train_n))))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPRCNeXd9TKu",
        "outputId": "787d00ab-7a63-4aba-def8-cbb8b7837dbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set score: 0.890\n",
            "Accuracy on test data: 84.43%\n",
            "Mean Cross-Validation, Kfold: 0.855\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.1 B Logistic Regression"
      ],
      "metadata": {
        "id": "pxZqBU2rq72p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# Define the parameter grid\n",
        "logreg_param_grid = {'C': [0.1, 1, 10], 'penalty': ['none', 'l1', 'l2']}\n",
        "\n",
        "# Create a logistic regression model\n",
        "logreg_n = LogisticRegression()\n",
        "\n",
        "# Create a grid search object\n",
        "logreg_grid_n = GridSearchCV(logreg_n, logreg_param_grid, cv=5)\n",
        "\n",
        "# Fit the grid search object to the data\n",
        "logreg_grid_n.fit(X_train_n, y_train_n)\n",
        "\n",
        "print(\"Best Parameter: {}\".format(logreg_grid_n.best_params_))\n",
        "print(\"Best Cross-Validation Score: {:.3f}\".format(logreg_grid_n.best_score_))\n",
        "print(\"Test set Score: {:.3f}\".format(logreg_grid_n.score(X_test_n, y_test_n)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYGQ60Jt9YgN",
        "outputId": "f8b238c9-985e-47cb-8d14-a974631fedf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameter: {'C': 0.1, 'penalty': 'none'}\n",
            "Best Cross-Validation Score: 0.841\n",
            "Test set Score: 0.836\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py:372: FitFailedWarning: \n",
            "15 fits failed out of a total of 45.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "15 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_validation.py\", line 680, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 1461, in fit\n",
            "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
            "  File \"/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py\", line 447, in _check_solver\n",
            "    raise ValueError(\n",
            "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/model_selection/_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.84130435        nan 0.82857143 0.84130435        nan 0.8378882\n",
            " 0.84130435        nan 0.84068323]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the logistic regression model\n",
        "logreg_n = LogisticRegression(C=0.1, penalty='none', random_state=42)\n",
        "\n",
        "# Train the model on the training data\n",
        "logreg_n.fit(X_train_n, y_train_n)\n",
        "print(\"Training set score: {:.3f}\".format(logreg_n.score(X_train_n, y_train_n)))\n",
        "\n",
        "# Evaluate the model on the test set  \n",
        "test_acc = logreg_n.score(X_test_n, y_test_n)\n",
        "print(\"Accuracy on test data: {:.2f}%\".format(test_acc * 100))\n",
        "\n",
        "from statistics import mean \n",
        "\n",
        "# Evaluate the model using k-fold cross-validation\n",
        "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(logreg_n, X_train_n, y_train_n))))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k26PuJde9Yuk",
        "outputId": "a6d712b4-04e4-434f-9c6b-fc21a9d4ad4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set score: 0.840\n",
            "Accuracy on test data: 83.64%\n",
            "Mean Cross-Validation, Kfold: 0.841\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:1483: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13.1 C Tree Classcifer"
      ],
      "metadata": {
        "id": "3cOWYvrPrA9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# create a Decision Tree Classifier object\n",
        "dtn = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# define the hyperparameter grid\n",
        "dt_params = {'criterion': ['gini', 'entropy'],\n",
        "          'max_depth': [3, 4, 5, 6, 7, 8, 9, 10],\n",
        "          'min_samples_split': [2, 3, 4, 5, 6, 7, 8, 9, 10]}\n",
        "\n",
        "# create a GridSearchCV object\n",
        "dtn_grid = GridSearchCV(estimator=dtn, param_grid=dt_params, cv=5, n_jobs=-1)\n",
        "\n",
        "# fit the GridSearchCV object to the data\n",
        "dtn_grid.fit(X_train_n, y_train_n)\n",
        "\n",
        "print(\"Best Parameter: {}\".format(dtn_grid.best_params_))\n",
        "print(\"Best Cross-Validation Score: {:.3f}\".format(dtn_grid.best_score_))\n",
        "print(\"Test set Score: {:.3f}\".format(dtn_grid.score(X_test_n, y_test_n)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUbYLZEJ9g1s",
        "outputId": "2c60099d-5d25-4587-e5cc-ab7526f208ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameter: {'criterion': 'entropy', 'max_depth': 6, 'min_samples_split': 2}\n",
            "Best Cross-Validation Score: 0.866\n",
            "Test set Score: 0.854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a decision tree classifier with the given parameters\n",
        "clfn = DecisionTreeClassifier(criterion='entropy', max_depth=6, min_samples_split=2)\n",
        "\n",
        "# Train the model on the training data\n",
        "clfn.fit(X_train_n, y_train_n)\n",
        "print(\"Training set score: {:.3f}\".format(clfn.score(X_train_n, y_train_n)))\n",
        "\n",
        "# Evaluate the model on the test set  \n",
        "test_acc = clfn.score(X_test_n, y_test_n)\n",
        "print(\"Accuracy on test data: {:.2f}%\".format(test_acc * 100))\n",
        "\n",
        "from statistics import mean \n",
        "\n",
        "# Evaluate the model using k-fold cross-validation\n",
        "print(\"Mean Cross-Validation, Kfold: {:.3f}\".format(np.mean(cross_val_score(clfn, X_train_n, y_train_n))))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0X__9AVJ9jw7",
        "outputId": "3bd08835-af51-4545-b18d-0e7aea98acc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set score: 0.884\n",
            "Accuracy on test data: 85.37%\n",
            "Mean Cross-Validation, Kfold: 0.865\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 13.2  and choose a final model. Why did you select this model among all of the models that you ran?  "
      ],
      "metadata": {
        "id": "M0YaZZOWqwW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The final model I chose is the random forest model with six variables, because this model has the highest score in both test data and cross validation. "
      ],
      "metadata": {
        "id": "fkd-zLwVhCG9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 14. What variable that currently is not in your model, if included, would be likely to increase your final model's predictive power?  For this answer try to speculate about a variable outside the variables available in the data that would improve you model."
      ],
      "metadata": {
        "id": "4SFPGg-qrJnU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I assume that the variable \"word_freq_buy\" might increase the final model's predictive power as spam emails usually promote products and entice people to buy them. "
      ],
      "metadata": {
        "id": "cKVC5SochBo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15. Lastly, you have listed each of the models that we have learned to use to predict dependent variables like spam.  List each model we have focused on in class thus far that you could use to evaluate data with a continuous dependent variable. "
      ],
      "metadata": {
        "id": "oGJ9dL0SrLnT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Supervised Learning Models:\n",
        "1. K Nearest Neighbors:KNN\n",
        "2. Ridge Regression\n",
        "3. Lasso Regression\n",
        "4. Linear Regression\n",
        "5. Logistic Regression\n",
        "6. Support Vector Machines\n",
        "7. Random Forests\n",
        "8. Decision Tree\n",
        "9. Bagging"
      ],
      "metadata": {
        "id": "t5bWggbKg_wH"
      }
    }
  ]
}